{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1621814864749
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_from_disk\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1621814207426
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# download model (cache)\n",
        "#model_name = \"google/pegasus-xsum\"\n",
        "#model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir = \"./cache\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir = \"./cache\")\n",
        "\n",
        "# save model\n",
        "#model.save_pretrained(\"../model/pegasus-xsum\")\n",
        "#tokenizer.save_pretrained(\"../model/pegasus-xsum\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1621814945775
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "model_path = \"../model/pegasus-xsum/\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1621814949309
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'document': 'The Welsh Economy Research report showed 79% of direct spend was retained in Wales, and associations built nearly 2,000 affordable homes.\\nThis was an increase of 4% on the previous year.\\nThe annual report, commissioned by Community Housing Cymru, looked at the impact of social housing in Wales.\\n£1.1bn\\ncontributed to the economy in 2014/15\\n£872m of that was retained in Wales\\n1,923 new homes built in 2014/15\\n£301m on repairs/maintenance in 2014/15\\n£532m on regeneration in 2014/15',\n",
              " 'id': '34846955',\n",
              " 'summary': 'Welsh housing associations directly contributed more than £1bn to the economy in 2014/15, an independent report has said.'}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = load_from_disk(\"../data/xsum\")\n",
        "test_data = data[\"test\"].select(range(1000, 1004))\n",
        "test_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1621815461572
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'summary_text': 'Social housing contributed £1.1bn to the Welsh economy in 2014/15, according to a new report.'},\n",
              " {'summary_text': 'The Foreign Office is \"urgently\" working with the authorities in Thailand to establish whether a British national has died.'},\n",
              " {'summary_text': 'Plans for a new school campus in the Scottish Borders have moved a step closer.'},\n",
              " {'summary_text': 'A woman has been taken to hospital following a one-vehicle crash in Aberdeenshire.'}]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# transformers v4.6.1\n",
        "# inference pipeline\n",
        "from transformers import SummarizationPipeline\n",
        "\n",
        "summarizer = SummarizationPipeline(model=model, tokenizer=tokenizer)\n",
        "summarizer(test_data[\"document\"], min_length=5, max_length=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1621815322856
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"The MSI GeForce 3080 Ti graphics card is now available to pre-order from the company's website.\"]\n"
          ]
        }
      ],
      "source": [
        "# inference wip\n",
        "#from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "\n",
        "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "#model_name = \"google/pegasus-xsum\"\n",
        "#model = PegasusForConditionalGeneration.from_pretrained(model_name, cache_dir = \"./cache\").to(device)\n",
        "#tokenizer = PegasusTokenizer.from_pretrained(model_name, cache_dir = \"./cache\")\n",
        "\n",
        "src_text = (\n",
        "\"It's no secret that NVIDIA is on the verge of releasing the GeForce RTX 3070 Ti and the GeForce RTX 3080 Ti. \"\n",
        "\"Despite the continued supply issues with both non-Ti versions of these cards, NVIDIA’s performance train must keep moving. \"\n",
        "\"However, it looks as though MSI jumped the gun a bit earlier today when it updated its website to include a product category \"\n",
        "\"for its upcoming GeForce RTX 3080 Ti graphics card family.\"\n",
        ")\n",
        "\n",
        "inputs = tokenizer([src_text], max_length=512, truncation=True, return_tensors='pt')\n",
        "summary_ids = model.generate(inputs['input_ids'])\n",
        "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1621815232160
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The MSI GeForce 3080 Ti graphics card is now available to pre-order from the company's website.\n"
          ]
        }
      ],
      "source": [
        "batch = tokenizer(src_text, truncation=True, padding=\"longest\", return_tensors='pt').to(device)\n",
        "translated = model.generate(**batch)\n",
        "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "print(tgt_text[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1621815290168
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The MSI GeForce 3080 Ti graphics card is now available to pre-order from the company's website.\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer.encode(src_text, max_length=512, truncation=True, return_tensors='pt')\n",
        "outputs = model.generate(inputs, max_length=64, min_length=10, length_penalty=0.8, num_beams=8, early_stopping=True)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1621815364559
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# inference comparison\n",
        "def summarize(dataset, model):\n",
        "    inputs = tokenizer(\n",
        "        dataset[\"document\"],\n",
        "        truncation=True,\n",
        "        max_length = 512, #max_source_length #encoder_max_length\n",
        "        padding=True,\n",
        "        return_tensors = \"pt\"\n",
        "    )\n",
        "    input_ids = inputs.input_ids.to(model.device) # docs\n",
        "    attention_mask = inputs.attention_mask.to(model.device) # docs\n",
        "    outputs = model.generate(input_ids, attention_mask=attention_mask) # docs\n",
        "    summary_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    return outputs, summary_texts\n",
        "\n",
        "#base_model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
        "\n",
        "#base_summaries = summarize(test_data, base_model)[1]\n",
        "finetuned_summaries = summarize(test_data, model)[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1621815367726
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source:\n",
            "Local reports from the southern resort island of Phuket say a British man died after turning a pistol on himself.\n",
            "The Foreign Office (FO) could not confirm the reports.\n",
            "An FO spokesman said: \"We are urgently working with the authorities in Thailand to establish whether a British national has died in Phuket.\"\n",
            "The Bangkok Post, quoted a taxi driver who said the man had hailed him near a local resort and asked to be taken somewhere where he could shoot. He had shown no signs of stress while in the taxi, the driver said.\n",
            "\n",
            "Target:\n",
            "The Foreign Office says it is urgently investigating reports that a British man has died at a shooting range in Thailand.\n",
            "\n",
            "Fine-tuned:\n",
            "The Foreign Office is \"urgently\" working with the authorities in Thailand to establish whether a British national has died.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "i = 1\n",
        "print(\"Source:\\n\" + test_data[\"document\"][i] + \"\\n\")\n",
        "print(\"Target:\\n\" + test_data[\"summary\"][i] + \"\\n\")\n",
        "print(\"Fine-tuned:\\n\" + finetuned_summaries[i] + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
