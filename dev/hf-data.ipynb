{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AutoConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1622247175911
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# WARNING: long process (xsum, cnn)\r\n",
    "#reddit = load_dataset(\"reddit\", cache_dir=\"../cache\")\r\n",
    "#cnn = load_dataset(\"cnn_dailymail\", \"3.0.0\", cache_dir=\"../cache\")\r\n",
    "#reddit_tifu = load_dataset(\"reddit_tifu\", \"long\", cache_dir=\"../cache\")\r\n",
    "#samsum = load_dataset(\"samsum\", cache_dir=\"../cache\")\r\n",
    "#cnn.save_to_disk(\"../data/cnn-dm\")\r\n",
    "#reddit_tifu.save_to_disk(\"../data/reddit-tifu\")\r\n",
    "#samsum.save_to_disk(\"../data/samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samsum = load_from_disk(\"../data/samsum\")\n",
    "\n",
    "model_name = \"facebook/bart-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir = \"./cache\")\n",
    "\n",
    "samsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_source_length = 512\n",
    "max_target_length = 64\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"dialogue\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, truncation=True, return_length=True, return_overflowing_tokens=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True, return_length=True, return_overflowing_tokens=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    print(model_inputs)\n",
    "    print(max(labels.length))\n",
    "    print(max(model_inputs.length))\n",
    "    print(max(model_inputs.overflowing_tokens))\n",
    "    print(max(model_inputs.num_truncated_tokens))\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "test = samsum[\"test\"].map(preprocess_function, batched=True)\n",
    "\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test[\"input_ids\"]))\n",
    "print(len(test[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1620687309179
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# reddit\r\n",
    "# features: ['author', 'subreddit', 'summary', 'text']\r\n",
    "# num_rows: 3848330\r\n",
    "#reddit = reddit[\"train\"].remove_columns([\"id\", \"subreddit_id\", \"body\", \"normalizedBody\"])\r\n",
    "#reddit = reddit.rename_column(\"content\", \"text\")\r\n",
    "#reddit.save_to_disk(\"../data/reddit\")\r\n",
    "#reddit = load_from_disk(\"../data/reddit\")\r\n",
    "#reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1620765475840
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# reddit-dota\r\n",
    "# features: ['author', 'summary', 'text']\r\n",
    "# num_rows: 22366\r\n",
    "#dota = reddit.filter(lambda x: x[\"subreddit\"] == \"DotA2\")\r\n",
    "#dota = dota.remove_columns(\"subreddit\")\r\n",
    "\r\n",
    "# bad text/summary @ dota[9]\r\n",
    "#dota = dota.filter(lambda x: len(x[\"text\"]) > len(x[\"summary\"])) # source length > target length\r\n",
    "#dota.flatten_indices().save_to_disk(\"../data/reddit-dota\")\r\n",
    "dota = load_from_disk(\"../data/reddit-dota\")\r\n",
    "dota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1620765545591
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# reddit-dota-proc\r\n",
    "# num_rows: 19949\r\n",
    "#max_text_length = 50000\r\n",
    "min_summary_length = 20\r\n",
    "max_ratio = 0.5\r\n",
    "min_ratio = 0.005\r\n",
    "#dota = dota.filter(lambda x: len(x[\"text\"]) <= max_text_length)\r\n",
    "proc = dota.filter(lambda x: len(x[\"summary\"]) >= min_summary_length)\r\n",
    "proc = proc.filter(lambda x: len(x[\"summary\"])/len(x[\"text\"]) <= max_ratio)\r\n",
    "proc = proc.filter(lambda x: len(x[\"summary\"])/len(x[\"text\"]) >= min_ratio)\r\n",
    "# bertscore filter?\r\n",
    "proc.flatten_indices().save_to_disk(\"../data/reddit-dota-proc\")\r\n",
    "proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1620765612229
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# reddit-dota-qtr\r\n",
    "# num_rows: 14389\r\n",
    "max_text_length = 3200\r\n",
    "min_summary_length = 30\r\n",
    "# 0.25(max: 128/512) & 0.125(max: 64/512)\r\n",
    "max_ratio = 0.25\r\n",
    "min_ratio = 0.01\r\n",
    "dota = load_from_disk(\"../data/reddit-dota-proc\")\r\n",
    "dota = dota.filter(lambda x: len(x[\"text\"]) <= max_text_length)\r\n",
    "dota = dota.filter(lambda x: len(x[\"summary\"]) >= min_summary_length)\r\n",
    "dota = dota.filter(lambda x: len(x[\"summary\"])/len(x[\"text\"]) <= max_ratio)\r\n",
    "dota = dota.filter(lambda x: len(x[\"summary\"])/len(x[\"text\"]) >= min_ratio)\r\n",
    "dota.flatten_indices().save_to_disk(\"../data/reddit-dota-qtr\")\r\n",
    "dota = load_from_disk(\"../data/reddit-dota-qtr\")\r\n",
    "dota"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
