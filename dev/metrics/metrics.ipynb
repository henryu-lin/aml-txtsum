{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## AzureML"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "from azureml.core import Workspace, Experiment, Run"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.scriptrun = azureml.core.script_run:ScriptRun._from_run_dto with exception (ruamel.yaml 0.17.16 (x:\\anaconda\\lib\\site-packages), Requirement.parse('ruamel.yaml<0.17.5,>=0.15.35')).\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "sub_id = \"6560575d-fa06-4e7d-95fb-f962e74efd7a\"\r\n",
        "resource_group = \"UW-Embeddings\"\r\n",
        "ws_name = \"TxtsumDev\"\r\n",
        "\r\n",
        "ws = Workspace.get(\r\n",
        "    name=ws_name,\r\n",
        "    subscription_id=sub_id,\r\n",
        "    resource_group=resource_group\r\n",
        ")\r\n",
        "\r\n",
        "experiment_name = \"hf-pytorch-demo\"\r\n",
        "run_id = \"bart-samsum-pytorch\""
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: Falling back to use azure cli login credentials.\n",
            "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
            "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "experiments = Experiment.list(ws)\r\n",
        "print(experiments)\r\n",
        "\r\n",
        "experiment = Experiment(workspace=ws, name=experiment_name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Experiment(Name: hf-deepspeed-demo,\n",
            "Workspace: TxtsumDev), Experiment(Name: hf-pytorch-demo,\n",
            "Workspace: TxtsumDev), Experiment(Name: hf-sweep-demo,\n",
            "Workspace: TxtsumDev), Experiment(Name: hf-test,\n",
            "Workspace: TxtsumDev)]\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "runs = Run.list(experiment)\r\n",
        "for run in runs:\r\n",
        "    print(run.id)\r\n",
        "run = Run(experiment, run_id)\r\n",
        "print(run)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08108a85-7197-4388-b05f-923c0b509e10\n",
            "1178575243\n",
            "17\n",
            "08c1ae08-4f66-419c-99e1-8f9f98d42e3c\n",
            "greenai\n",
            "bart-samsum-pytorch\n",
            "Run(Experiment: hf-pytorch-demo,\n",
            "Id: bart-samsum-pytorch,\n",
            "Type: azureml.scriptrun,\n",
            "Status: Completed)\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "metrics = run.get_metrics()\r\n",
        "print(metrics.keys())\r\n",
        "\r\n",
        "for k in metrics.keys():\r\n",
        "    if \"rouge\" in k:\r\n",
        "        print(f\"{k}: {metrics.get(k)[-1]}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['loss', 'learning_rate', 'epoch', 'eval_loss', 'eval_rouge1', 'eval_rouge2', 'eval_rougeL', 'eval_rougeLsum', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'train_runtime', 'train_samples_per_second', 'train_steps_per_second', 'total_flos', 'train_loss'])\n",
            "eval_rouge1: 54.6725\n",
            "eval_rouge2: 29.7084\n",
            "eval_rougeL: 45.0512\n",
            "eval_rougeLsum: 50.5033\n"
          ]
        }
      ],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628659871218
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "run_details = run.get_details()\r\n",
        "run_details"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "command = run_details.get(\"runDefinition\").get(\"command\")\r\n",
        "command_args = command.rstrip(\"\\n\").replace(\" \", \": \").split(\": --\")\r\n",
        "\r\n",
        "# temp args to ignore\r\n",
        "ignore_args = (\r\n",
        "    \"python\",\r\n",
        "    \"deepspeed\",\r\n",
        "    \"model_name_or_path\",\r\n",
        "    \"config_name\",\r\n",
        "    \"dataset_name\",\r\n",
        "    \"dataset_path\",\r\n",
        "    \"evaluation_strategy\",\r\n",
        "    \"logging_strategy\",\r\n",
        "    \"do_train\",\r\n",
        "    \"do_eval\",\r\n",
        "    \"do_predict\",\r\n",
        "    \"predict_with_generate\",\r\n",
        "    \"overwrite_output_dir\",\r\n",
        "    \"output_dir\",\r\n",
        "    \"logging_dir\",\r\n",
        "    \"ddp_find_unused_parameters\"\r\n",
        ")\r\n",
        "\r\n",
        "hyperparams = \"\\n\".join(f\"{arg}\" for arg in command_args if not any(i in arg for i in ignore_args))\r\n",
        "\r\n",
        "print(hyperparams)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_source_length: 512\n",
            "max_target_length: 90\n",
            "fp16: True\n",
            "seed: 1\n",
            "per_device_train_batch_size: 16\n",
            "per_device_eval_batch_size: 16\n",
            "learning_rate: 5e-5\n",
            "weight_decay: 0.1\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "# check formatting\r\n",
        "hyperparam_md = f\"\"\"## Hyperparameters\r\n",
        "```yaml\r\n",
        "{hyperparams}\r\n",
        "```\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "print(hyperparam_md)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Hyperparameters\n",
            "```yaml\n",
            "max_source_length: 512\n",
            "max_target_length: 90\n",
            "fp16: True\n",
            "seed: 1\n",
            "per_device_train_batch_size: 16\n",
            "per_device_eval_batch_size: 16\n",
            "learning_rate: 5e-5\n",
            "weight_decay: 0.1\n",
            "```\n",
            "\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "compute_details = {\r\n",
        "    \"size\": ws.compute_targets.get(run_details.get(\"target\")).vm_size,\r\n",
        "    \"node_count\": run_details.get(\"runDefinition\").get(\"nodeCount\")\r\n",
        "}"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "# region = westus2\r\n",
        "# gpu device, dedicated, low priority ($/hr)\r\n",
        "sku_mapping = {\r\n",
        "    \"STANDARD_ND96ASR_V4\": (\"8 x NVIDIA A100 40GB (NVLink 3.0)\", 27.20, 5.44),\r\n",
        "    \"STANDARD_ND40RS_V2\": (\"8 x NVIDIA V100 32GB (NVLink)\", 22.03, 4.41),\r\n",
        "    \"STANDARD_NC24S_V3\": (\"4 x NVIDIA V100 16GB\", 12.24, 2.45),\r\n",
        "    \"STANDARD_NC6\": (\"1 x NVIDIA K80 12GB\", 0.90, 0.18)\r\n",
        "}\r\n",
        "\r\n",
        "compute_table = f\"\"\"| Region | US West 2 |\r\n",
        "| AzureML Compute SKU | {compute_details[\"size\"]} |\r\n",
        "| Compute SKU GPU Device | {sku_mapping.get(compute_details[\"size\"])[0]} |\r\n",
        "| Compute Node Count | {compute_details[\"node_count\"]} |\r\n",
        "\"\"\"\r\n",
        "print(compute_table)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "| AzureML Compute SKU | STANDARD_ND40RS_V2 |\n",
            "| Compute SKU GPU Device | 8 X V100 32GB |\n",
            "| Compute Node Count | 1 |\n",
            "\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Azure Monitor"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "#%pip install azure-identity\r\n",
        "#%pip install azure-mgmt-monitor\r\n",
        "import datetime\r\n",
        "from azure.mgmt.monitor import MonitorManagementClient\r\n",
        "from azure.identity import AzureCliCredential#, DefaultAzureCredential"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "ws_resource_id = (\r\n",
        "    f\"subscriptions/{sub_id}/\"\r\n",
        "    f\"resourceGroups/{resource_group}/\"\r\n",
        "    f\"providers/Microsoft.MachineLearningServices/workspaces/{ws_name}\"\r\n",
        ")\r\n",
        "\r\n",
        "monitor_client = MonitorManagementClient(AzureCliCredential(), sub_id)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "#today = datetime.datetime.now().date()\r\n",
        "#start_date = today - datetime.timedelta(days=7)\r\n",
        "\r\n",
        "start_datetime = datetime.datetime.strptime(run_details.get(\"startTimeUtc\"), \"%Y-%m-%dT%H:%M:%S.%fZ\")\r\n",
        "end_datetime = datetime.datetime.strptime(run_details.get(\"endTimeUtc\"), \"%Y-%m-%dT%H:%M:%S.%fZ\")\r\n",
        "\r\n",
        "metrics_data = monitor_client.metrics.list(\r\n",
        "    ws_resource_id,\r\n",
        "    # add 1 min buffer to end time, td: hr/d depending on interval\r\n",
        "    timespan=f\"{start_datetime}/{end_datetime + datetime.timedelta(minutes=1)}\",\r\n",
        "    interval=\"PT1M\",\r\n",
        "    metricnames=\"GpuEnergyJoules\",\r\n",
        "    aggregation=\"Total\",\r\n",
        "    filter=f\"RunID eq '{run_id}'\"\r\n",
        ")\r\n",
        "\r\n",
        "for item in metrics_data.value:\r\n",
        "    print(f\"| Timestamp (RFC 3339) | {item.name.localized_value} |\")\r\n",
        "    for timeserie in item.timeseries:\r\n",
        "        for data in timeserie.data:\r\n",
        "            print(f\"| {data.time_stamp} | {data.total} |\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Timestamp (RFC 3339) | GpuEnergyJoules |\n",
            "| 2021-08-29 01:46:00+00:00 | 0.0 |\n",
            "| 2021-08-29 01:47:00+00:00 | 0.0 |\n",
            "| 2021-08-29 01:48:00+00:00 | 0.0 |\n",
            "| 2021-08-29 01:49:00+00:00 | 20673.0 |\n",
            "| 2021-08-29 01:50:00+00:00 | 58252.0 |\n",
            "| 2021-08-29 01:51:00+00:00 | 96313.0 |\n",
            "| 2021-08-29 01:52:00+00:00 | 72549.0 |\n",
            "| 2021-08-29 01:53:00+00:00 | 78087.0 |\n",
            "| 2021-08-29 01:54:00+00:00 | 108031.0 |\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "run_duration = end_datetime - start_datetime\r\n",
        "\r\n",
        "ts = run_duration.seconds\r\n",
        "\r\n",
        "m = ts // 60\r\n",
        "s = ts % 60\r\n",
        "print(f\"{m}m{s}s\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8m28s\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "# cost = sku_price * nodes * run_duration\r\n",
        "compute_cost = []\r\n",
        "compute_cost.append(sku_mapping.get(compute_details[\"size\"])[1] * compute_details[\"node_count\"] * ts / 3600)\r\n",
        "compute_cost.append(sku_mapping.get(compute_details[\"size\"])[2] * compute_details[\"node_count\"] * ts / 3600)\r\n",
        "\"${:,.2f}\".format(compute_cost[0])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'$3.11'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "print(f\"\"\"| Run Duration | {m}m {s}s |\r\n",
        "| Compute Cost (LowPriority/Dedicated) | ${\"{:,.2f}\".format(compute_cost[0])} / ${\"{:,.2f}\".format(compute_cost[1])} USD |\r\n",
        "\"\"\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Region | US West 2 |\n",
            "| Run Duration | 8m28s |\n",
            "| Compute Cost (LowPriority/Dedicated) | $3.11 / $0.62 USD |\n",
            "\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "monitor_metrics = {\r\n",
        "    \"CpuUtilizationPercentage\": \"Average\",\r\n",
        "    \"GpuUtilizationPercentage\": \"Average\",\r\n",
        "    \"GpuMemoryUtilizationMegabytes\": \"Average\",\r\n",
        "    \"GpuEnergyJoules\": \"Total\"\r\n",
        "}\r\n",
        "\r\n",
        "monitor_results = []\r\n",
        "for k, v in monitor_metrics.items():\r\n",
        "    results = monitor_client.metrics.list(\r\n",
        "        ws_resource_id,\r\n",
        "        # add 1 min buffer to end time, td: hr/d depending on interval\r\n",
        "        timespan=f\"{start_datetime}/{end_datetime + datetime.timedelta(minutes=1)}\",\r\n",
        "        interval=\"P1D\",\r\n",
        "        metricnames=k,\r\n",
        "        aggregation=v,\r\n",
        "        filter=f\"RunID eq '{run_id}'\"\r\n",
        "    )\r\n",
        "\r\n",
        "    for item in results.value:\r\n",
        "        print(f\"| {item.name.localized_value} ({v}) |\")\r\n",
        "        for timeserie in item.timeseries:\r\n",
        "            for data in timeserie.data:\r\n",
        "                monitor_results.append(getattr(data, v.lower()))\r\n",
        "                print(f\"| {getattr(data, v.lower())} |\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| CpuUtilizationPercentage (Average) |\n",
            "| 40.5 |\n",
            "| GpuUtilizationPercentage (Average) |\n",
            "| 59.9 |\n",
            "| GpuMemoryUtilizationMegabytes (Average) |\n",
            "| 20681.5 |\n",
            "| GpuEnergyJoules (Total) |\n",
            "| 325874.0 |\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "monitor_table = f\"\"\"| Average CPU Utilization | {\"{:,.1f}\".format(monitor_results[0])}% |\r\n",
        "| Average GPU Utilization | {\"{:,.1f}\".format(monitor_results[1])}% |\r\n",
        "| Average GPU Memory Usage | {\"{:,.2f}\".format(monitor_results[2]/1000)} GB |\r\n",
        "| Total GPU Energy Usage | {\"{:,.2f}\".format(monitor_results[3]/1000)} kJ |\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "print(monitor_table)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Average CPU Utilization | 40.5% |\n",
            "| Average GPU Utilization | 59.9% |\n",
            "| Average GPU Memory Usage | 20.68 GB |\n",
            "| Total GPU Energy Usage | 325.87 kJ |\n",
            "\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLflow"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import mlflow\r\n",
        "from mlflow.tracking import MlflowClient"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\r\n",
        "client = MlflowClient()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "experiments = client.list_experiments()\r\n",
        "#experiment = client.get_experiment_by_name(\"hf-pytorch-demo\")\r\n",
        "print(experiments)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628698175887
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "runs = client.list_run_infos(experiments[1].experiment_id)\r\n",
        "run = client.get_run(runs[0].run_id)\r\n",
        "print(runs)\r\n",
        "print(run)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628698806233
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def print_metric_info(history):\r\n",
        "    for m in history:\r\n",
        "        print(f\"name: {m.key}\")\r\n",
        "        print(f\"value: {m.value}\")\r\n",
        "        print(f\"step: {m.step}\")\r\n",
        "        print(f\"timestamp (unix ms): {m.timestamp}\")\r\n",
        "        print(\"--\")\r\n",
        "\r\n",
        "metrics = [\"eval_rouge1\", \"eval_rouge2\", \"eval_rougeL\"]\r\n",
        "for metric in metrics:\r\n",
        "    run_metrics = client.get_metric_history(run.info.run_id, metric)\r\n",
        "    print_metric_info(run_metrics)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628698125652
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit (conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "interpreter": {
      "hash": "494b91ccaf2bd8f9cafb9dabefa1bea598e6699f88d192a5dd69e6dde3c6251e"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}