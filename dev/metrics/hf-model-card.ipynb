{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# td: parse args, refactor\r\n",
        "from azureml.core import Workspace, Experiment, Run\r\n",
        "\r\n",
        "sub_id = \"6560575d-fa06-4e7d-95fb-f962e74efd7a\"\r\n",
        "resource_group = \"UW-Embeddings\"\r\n",
        "workspace_name = \"TxtsumDemo\"\r\n",
        "\r\n",
        "experiment_name = \"hf-pytorch-demo\"\r\n",
        "run_id = \"bart-samsum-pytorch\"\r\n",
        "\r\n",
        "ws = Workspace.get(\r\n",
        "    name=workspace_name,\r\n",
        "    subscription_id=sub_id,\r\n",
        "    resource_group=resource_group\r\n",
        ")\r\n",
        "\r\n",
        "experiment = Experiment(ws, experiment_name)\r\n",
        "run = Run(experiment, run_id)\r\n",
        "run_details = run.get_details()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "source": [
        "command = run_details.get(\"runDefinition\").get(\"command\")\r\n",
        "command_args = command.rstrip(\"\\n\").replace(\" \", \": \").split(\": --\")\r\n",
        "\r\n",
        "# temp args to ignore\r\n",
        "ignore_args = (\r\n",
        "    \"python\",\r\n",
        "    \"deepspeed\",\r\n",
        "    \"model_name_or_path\",\r\n",
        "    \"config_name\",\r\n",
        "    \"dataset_name\",\r\n",
        "    \"dataset_path\",\r\n",
        "    \"evaluation_strategy\",\r\n",
        "    \"logging_strategy\",\r\n",
        "    \"save_strategy\",\r\n",
        "    \"do_train\",\r\n",
        "    \"do_eval\",\r\n",
        "    \"do_predict\",\r\n",
        "    \"predict_with_generate\",\r\n",
        "    \"overwrite_output_dir\",\r\n",
        "    \"output_dir\",\r\n",
        "    \"logging_dir\",\r\n",
        "    \"ddp_find_unused_parameters\"\r\n",
        ")\r\n",
        "\r\n",
        "hyperparams = \"\\n\".join(f\"- {arg}\" for arg in command_args if not any(i in arg for i in ignore_args))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "source": [
        "compute_details = {\r\n",
        "    \"size\": ws.compute_targets.get(run_details.get(\"target\")).vm_size,\r\n",
        "    \"node_count\": run_details.get(\"runDefinition\").get(\"nodeCount\")\r\n",
        "}\r\n",
        "\r\n",
        "# region = westus2\r\n",
        "# gpu device, dedicated, low priority ($/hr)\r\n",
        "sku_mapping = {\r\n",
        "    \"STANDARD_ND96ASR_V4\": (\"8 x NVIDIA A100 40GB (NVLink 3.0)\", 27.20, 5.44),\r\n",
        "    \"STANDARD_ND40RS_V2\": (\"8 x NVIDIA V100 32GB (NVLink)\", 22.03, 4.41),\r\n",
        "    \"STANDARD_NC24S_V3\": (\"4 x NVIDIA V100 16GB\", 12.24, 2.45),\r\n",
        "    \"STANDARD_NC6\": (\"1 x NVIDIA K80 12GB\", 0.90, 0.18)\r\n",
        "}\r\n",
        "\r\n",
        "compute_table = f\"\"\"\r\n",
        "| Region | US West 2 |\r\n",
        "| AzureML Compute SKU | {compute_details[\"size\"]} |\r\n",
        "| Compute SKU GPU Device | {sku_mapping.get(compute_details[\"size\"])[0]} |\r\n",
        "| Compute Node Count | {compute_details[\"node_count\"]} |\r\n",
        "\"\"\".lstrip(\"\\n\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "source": [
        "#%pip install azure-identity\r\n",
        "#%pip install azure-mgmt-monitor\r\n",
        "import datetime\r\n",
        "from azure.mgmt.monitor import MonitorManagementClient\r\n",
        "from azure.identity import AzureCliCredential#, DefaultAzureCredential\r\n",
        "\r\n",
        "ws_resource_id = (\r\n",
        "    f\"subscriptions/{sub_id}/\"\r\n",
        "    f\"resourceGroups/{resource_group}/\"\r\n",
        "    f\"providers/Microsoft.MachineLearningServices/workspaces/{ws_name}\"\r\n",
        ")\r\n",
        "\r\n",
        "monitor_client = MonitorManagementClient(AzureCliCredential(), sub_id)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "source": [
        "start_datetime = datetime.datetime.strptime(run_details.get(\"startTimeUtc\"), \"%Y-%m-%dT%H:%M:%S.%fZ\")\r\n",
        "end_datetime = datetime.datetime.strptime(run_details.get(\"endTimeUtc\"), \"%Y-%m-%dT%H:%M:%S.%fZ\")\r\n",
        "\r\n",
        "run_duration = end_datetime - start_datetime\r\n",
        "\r\n",
        "# run duration in minutes, seconds\r\n",
        "ts = run_duration.seconds\r\n",
        "m = ts // 60\r\n",
        "s = ts % 60\r\n",
        "\r\n",
        "# cost = sku_price * nodes * run_duration\r\n",
        "compute_cost = []\r\n",
        "compute_cost.append(sku_mapping.get(compute_details[\"size\"])[1] * compute_details[\"node_count\"] * ts / 3600)\r\n",
        "compute_cost.append(sku_mapping.get(compute_details[\"size\"])[2] * compute_details[\"node_count\"] * ts / 3600)\r\n",
        "\r\n",
        "cost_table = f\"\"\"\r\n",
        "| Run Duration | {m}m {s}s |\r\n",
        "| Compute Cost (Dedicated/LowPriority) | ${\"{:,.2f}\".format(compute_cost[0])} / ${\"{:,.2f}\".format(compute_cost[1])} USD |\r\n",
        "\"\"\".lstrip(\"\\n\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "source": [
        "# metrics to retrieve & aggregation\r\n",
        "monitor_metrics = {\r\n",
        "    \"CpuUtilizationPercentage\": \"Average\",\r\n",
        "    \"GpuUtilizationPercentage\": \"Average\",\r\n",
        "    \"GpuMemoryUtilizationMegabytes\": \"Average\",\r\n",
        "    \"GpuEnergyJoules\": \"Total\"\r\n",
        "}\r\n",
        "\r\n",
        "monitor_results = []\r\n",
        "for k, v in monitor_metrics.items():\r\n",
        "    results = monitor_client.metrics.list(\r\n",
        "        ws_resource_id,\r\n",
        "        # add 1 min buffer to end time, td: hr/d depending on interval\r\n",
        "        timespan=f\"{start_datetime}/{end_datetime + datetime.timedelta(minutes=1)}\",\r\n",
        "        interval=\"P1D\",\r\n",
        "        metricnames=k,\r\n",
        "        aggregation=v,\r\n",
        "        filter=f\"RunID eq '{run_id}'\"\r\n",
        "    )\r\n",
        "\r\n",
        "    for item in results.value:\r\n",
        "        for timeserie in item.timeseries:\r\n",
        "            for data in timeserie.data:\r\n",
        "                monitor_results.append(getattr(data, v.lower()))\r\n",
        "\r\n",
        "monitor_table = f\"\"\"\r\n",
        "| Average CPU Utilization | {\"{:,.1f}\".format(monitor_results[0])}% |\r\n",
        "| Average GPU Utilization | {\"{:,.1f}\".format(monitor_results[1])}% |\r\n",
        "| Average GPU Memory Usage | {\"{:,.2f}\".format(monitor_results[2]/1000)} GB |\r\n",
        "| Total GPU Energy Usage | {\"{:,.2f}\".format(monitor_results[3]/1000)} kJ |\r\n",
        "\"\"\".lstrip(\"\\n\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "source": [
        "resource_table = f\"{compute_table}{cost_table}{monitor_table}\""
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from transformers import (\r\n",
        "    #AutoConfig,\r\n",
        "    AutoModelForSeq2SeqLM,\r\n",
        "    AutoTokenizer\r\n",
        ")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model_path = f\"./ExperimentRun/dcid.{run_id}/outputs\"\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\r\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625645430576
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "source": [
        "model_name = \"bart-large-samsum\"\r\n",
        "repo_id = \"linydub\""
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "source": [
        "# td: retrieve metrics from mlflow/aml instead, log predict/test metrics to mlflow\r\n",
        "import json\r\n",
        "\r\n",
        "# read trainer results\r\n",
        "rouge_results = {}\r\n",
        "metric_results = {}\r\n",
        "with open(f\"{model_path}/all_results.json\") as f:\r\n",
        "    results_raw = json.load(f)\r\n",
        "    for k in results_raw.keys():\r\n",
        "        if \"rouge\" in k:\r\n",
        "            rouge_results[k] = results_raw[k]\r\n",
        "        else:\r\n",
        "            metric_results[k] = results_raw[k]\r\n",
        "\r\n",
        "ts_results={}\r\n",
        "with open(f\"{model_path}/trainer_state.json\") as f:\r\n",
        "    trainer_state_raw = json.load(f)\r\n",
        "    ts_results[\"total_steps\"] = trainer_state_raw[\"max_steps\"]\r\n",
        "    ts_results[\"total_flops\"] = trainer_state_raw[\"total_flos\"]"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625638912497
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "source": [
        "# td: check if report_to codecarbon, log codecarbon metrics/params to mlflow\r\n",
        "import csv\r\n",
        "# read codecarbon results\r\n",
        "carbon_keys = [\"timestamp\", \"duration\", \"emissions\", \"energy_consumed\", \"country_name\", \"region\", \"cloud_provider\", \"cloud_region\"]\r\n",
        "with open(f\"{model_path}/emissions.csv\") as f:\r\n",
        "    emissions_raw = csv.DictReader(f)\r\n",
        "    emissions_results={}\r\n",
        "    for row in emissions_raw:\r\n",
        "        for k in carbon_keys:\r\n",
        "            emissions_results[k] = row[k]"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625638914050
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "source": [
        "# logged metric name format: {dataset_set}_{metric}\r\n",
        "def md_metrics(k, v, metrics):\r\n",
        "  if any(i in k for i in metrics):\r\n",
        "    dataset_set, metric = k.split(\"_\")\r\n",
        "    metric_name = \"\"\r\n",
        "\r\n",
        "    if \"train\" in dataset_set:\r\n",
        "      metric_name = \"Training \"\r\n",
        "    elif any(i in dataset_set for i in [\"eval\", \"val\"]):\r\n",
        "      metric_name = \"Validation \"\r\n",
        "    elif any(i in dataset_set for i in [\"test\", \"predict\"]):\r\n",
        "      metric_name = \"Test \"\r\n",
        "    \r\n",
        "    if (i := \"rouge\") in metrics:\r\n",
        "      metric = metric.replace(i, i + \"-\")\r\n",
        "      metric_name += metric.replace(i, i.upper())\r\n",
        "\r\n",
        "    return f\"\"\"\r\n",
        "    - name: {metric_name}\r\n",
        "      type: {metric}\r\n",
        "      value: {v}\"\"\".lstrip(\"\\n\")\r\n",
        "  else:\r\n",
        "    return \"\""
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "source": [
        "# td: generate arm template in outputs after run\r\n",
        "# td: generate & upload arm template to repo\r\n",
        "import urllib.parse\r\n",
        "\r\n",
        "def generate_arm_buttons(url):\r\n",
        "    encoded_url = urllib.parse.quote(url, safe='')\r\n",
        "\r\n",
        "    arm_deploy = f\"[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/{encoded_url})\"\r\n",
        "    arm_visualize = f\"[![Visualize](https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/1-CONTRIBUTION-GUIDE/images/visualizebutton.svg?sanitize=true)](http://armviz.io/#/?load={url})\"\r\n",
        "\r\n",
        "    return f\"{arm_deploy} {arm_visualize}\""
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "source": [
        "def md_table(results):\r\n",
        "    return \"\\n\".join(f\"| {k} | {v} |\" for k, v in results.items())"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# td: retrieve from config path or ds params\r\n",
        "deepspeed_config = \"\"\"### DeepSpeed Config\r\n",
        "Optimizer = `AdamW`, Scheduler = `WarmupDecayLR`, Offload = `none`\r\n",
        "```json\r\n",
        "  \"zero_optimization\": {\r\n",
        "    \"stage\": 2,\r\n",
        "    \"allgather_partitions\": true,\r\n",
        "    \"allgather_bucket_size\": 1000000000,\r\n",
        "    \"overlap_comm\": true,\r\n",
        "    \"reduce_scatter\": true,\r\n",
        "    \"reduce_bucket_size\": 1000000000,\r\n",
        "    \"contiguous_gradients\": true\r\n",
        "  }\r\n",
        "```\r\n",
        "\"\"\""
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "source": [
        "# generate model card (summarization)\r\n",
        "MODEL_CARD_TEMPLATE = \"\"\"\r\n",
        "---\r\n",
        "language:\r\n",
        "- en\r\n",
        "license: apache-2.0\r\n",
        "tags:\r\n",
        "{tag_list}\r\n",
        "datasets:\r\n",
        "- {dataset_id}\r\n",
        "metrics:\r\n",
        "{metric_list}\r\n",
        "model-index:\r\n",
        "- name: {model_id}\r\n",
        "  results:\r\n",
        "  - task: \r\n",
        "      name: Abstractive Text Summarization\r\n",
        "      type: abstractive-text-summarization\r\n",
        "    dataset:\r\n",
        "      name: \"{dataset_name}\" \r\n",
        "      type: {dataset_id}\r\n",
        "    metrics:\r\n",
        "{metric_details}\r\n",
        "widget:\r\n",
        "- text: | \r\n",
        "{sample_text}\r\n",
        "---\r\n",
        "\r\n",
        "## `{model_id}`\r\n",
        "This model was trained using Microsoft's [`Azure Machine Learning Service`](https://azure.microsoft.com/en-us/services/machine-learning). It was fine-tuned on the [`{dataset_id}`](https://huggingface.co/datasets/{dataset_id}) corpus from [`{base_model_id}`](https://huggingface.co/{base_model_id}) checkpoint.\r\n",
        "\r\n",
        "## Usage (Inference)\r\n",
        "```python\r\n",
        "from transformers import pipeline\r\n",
        "summarizer = pipeline(\"summarization\", model=\"{repo_id}/{model_id}\")\r\n",
        "\r\n",
        "input_text = '''\r\n",
        "{sample_text}\r\n",
        "'''\r\n",
        "summarizer(input_text)\r\n",
        "```\r\n",
        "\r\n",
        "## Reproduce this model on AzureML\r\n",
        "{arm_buttons}\r\n",
        "\r\n",
        "More information about the fine-tuning process (including samples and benchmarks):  \r\n",
        "**[Preview]** https://github.com/linydub/azureml-greenai-txtsum\r\n",
        "\r\n",
        "## Resource Usage\r\n",
        "These results were retrieved from [`Azure Monitor Metrics`](https://docs.microsoft.com/en-us/azure/azure-monitor/essentials/data-platform-metrics). All experiments were ran on AzureML low priority compute clusters.\r\n",
        "\r\n",
        "| Key | Value |\r\n",
        "| --- | ----- |\r\n",
        "{resource_table}\r\n",
        "\r\n",
        "*Compute cost ($) is estimated from the run duration, number of compute nodes utilized, and SKU's price per hour. Updated SKU pricing could be found [here](https://azure.microsoft.com/en-us/pricing/details/machine-learning).  \r\n",
        "\r\n",
        "### Carbon Emissions\r\n",
        "These results were obtained using [`CodeCarbon`](https://github.com/mlco2/codecarbon). The carbon emissions are estimated from training runtime only (excl. setup and evaluation runtimes).  \r\n",
        "\r\n",
        "| Key | Value |\r\n",
        "| --- | ----- |\r\n",
        "{emissions_table}\r\n",
        "\r\n",
        "## Hyperparameters\r\n",
        "\r\n",
        "{hyperparams}\r\n",
        "\r\n",
        "{deepspeed_config}\r\n",
        "\r\n",
        "## Results\r\n",
        "| ROUGE | Score |\r\n",
        "| ----- | ----- |\r\n",
        "{rouge_table}\r\n",
        "\r\n",
        "| Metric | Value |\r\n",
        "| ------ | ----- |\r\n",
        "{results_table}\r\n",
        "{ts_table}\r\n",
        "\"\"\".lstrip(\"\\n\")\r\n",
        "\r\n",
        "tags = [\"summarization\", \"azureml\", \"azure\", \"codecarbon\", \"bart\"]\r\n",
        "metrics = [\"rouge\"]\r\n",
        "arm_template_url = f\"https://raw.githubusercontent.com/linydub/azureml-greenai-txtsum/main/.cloud/template-hub/{repo_id}/arm-{model_name}.json\"\r\n",
        "\r\n",
        "model_card = MODEL_CARD_TEMPLATE.format(\r\n",
        "    rouge_table=md_table(rouge_results),\r\n",
        "    results_table=md_table(metric_results),\r\n",
        "    ts_table=md_table(ts_results),\r\n",
        "    emissions_table=md_table(emissions_results),\r\n",
        "    tag_list=\"\\n\".join(f\"- {tag}\" for tag in tags),\r\n",
        "    metric_list=\"\\n\".join(f\"- {metric}\" for metric in metrics),\r\n",
        "    metric_details=\"\\n\".join(md_metrics(k, v, metrics) for k, v in rouge_results.items()),\r\n",
        "    sample_text=\"\"\"\r\n",
        "    Henry: Hey, is Nate coming over to watch the movie tonight?\r\n",
        "    Kevin: Yea, he said he'll be arriving a bit later at around 7 since he gets off of work at 6. Have you taken out the garbage yet?\r\n",
        "    Henry: Oh I forgot. I'll do that once I'm finished with my assignment for my math class.\r\n",
        "    Kevin: Yea, you should take it out as soon as possible. And also, Nate is bringing his girlfriend.\r\n",
        "    Henry: Nice, I'm really looking forward to seeing them again.\"\"\".lstrip(\"\\n\"),\r\n",
        "    repo_id=repo_id,\r\n",
        "    model_id=model_name,\r\n",
        "    base_model_id=\"facebook/bart-large\",\r\n",
        "    dataset_name=\"SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization\",\r\n",
        "    dataset_id=\"samsum\",\r\n",
        "    arm_buttons=generate_arm_buttons(arm_template_url),\r\n",
        "    resource_table=resource_table,\r\n",
        "    hyperparams=hyperparams,\r\n",
        "    deepspeed_config=\"\"\r\n",
        ")\r\n",
        "\r\n",
        "with open(f\"{model_path}/README.md\", \"w\") as f:\r\n",
        "    f.write(model_card)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625639262595
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#%curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\r\n",
        "#%sudo apt-get install git-lfs\r\n",
        "#%git lfs install\r\n",
        "#%transformers-cli login"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "#model.push_to_hub(model_name, organization=repo_id)\r\n",
        "#tokenizer.push_to_hub(model_name, organization=repo_id)\r\n",
        "\r\n",
        "model.push_to_hub(model_name)\r\n",
        "tokenizer.push_to_hub(model_name)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625638890541
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit (conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "interpreter": {
      "hash": "494b91ccaf2bd8f9cafb9dabefa1bea598e6699f88d192a5dd69e6dde3c6251e"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}