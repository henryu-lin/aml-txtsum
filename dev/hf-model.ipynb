{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!transformers-cli login\r\n",
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\r\n",
        "!sudo apt-get install git-lfs\r\n",
        "!git lfs install"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!az ml datastore download -n txtsumstorage_azureml -d ../ -p ExperimentRun/dcid.5f86907b-2602-4a2a-b547-4aa8d5493368/outputs"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\r\n",
        "    AutoConfig,\r\n",
        "    AutoModelForSeq2SeqLM,\r\n",
        "    AutoTokenizer\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1625644962122
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"../ExperimentRun/t5-3b-samsum-deepspeed/outputs\"\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\r\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625645430576
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_n = \"t5-3b-samsum-deepspeed\"\r\n",
        "model.push_to_hub(model_n)\r\n",
        "tokenizer.push_to_hub(model_n)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625638890541
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\r\n",
        "# read trainer results \r\n",
        "with open(f\"{model_path}/eval_results.json\") as f:\r\n",
        "    eval_results_raw = json.load(f)\r\n",
        "    eval_results={}\r\n",
        "    eval_results[\"eval_rouge1\"] = eval_results_raw[\"eval_rouge1\"]\r\n",
        "    eval_results[\"eval_rouge2\"] = eval_results_raw[\"eval_rouge2\"]\r\n",
        "    eval_results[\"eval_rougeL\"] = eval_results_raw[\"eval_rougeL\"]\r\n",
        "    eval_results[\"eval_rougeLsum\"] = eval_results_raw[\"eval_rougeLsum\"]\r\n",
        "\r\n",
        "with open(f\"{model_path}/predict_results.json\") as f:\r\n",
        "    test_results_raw = json.load(f)\r\n",
        "    test_results={}\r\n",
        "    test_results[\"predict_rouge1\"] = test_results_raw[\"predict_rouge1\"]\r\n",
        "    test_results[\"predict_rouge2\"] = test_results_raw[\"predict_rouge2\"]\r\n",
        "    test_results[\"predict_rougeL\"] = test_results_raw[\"predict_rougeL\"]\r\n",
        "    test_results[\"predict_rougeLsum\"] = test_results_raw[\"predict_rougeLsum\"]\r\n",
        "\r\n",
        "with open(f\"{model_path}/all_results.json\") as f:\r\n",
        "    results_raw = json.load(f)\r\n",
        "    all_results={}\r\n",
        "    all_results[\"eval_gen_len\"] = results_raw[\"eval_gen_len\"]\r\n",
        "    all_results[\"predict_gen_len\"] = results_raw[\"predict_gen_len\"]\r\n",
        "    all_results[\"train_loss\"] = results_raw[\"train_loss\"]\r\n",
        "    all_results[\"eval_loss\"] = results_raw[\"eval_loss\"]\r\n",
        "    all_results[\"predict_loss\"] = results_raw[\"predict_loss\"]\r\n",
        "    all_results[\"train_runtime\"] = results_raw[\"train_runtime\"]\r\n",
        "    all_results[\"train_samples\"] = results_raw[\"train_samples\"]\r\n",
        "    all_results[\"train_samples_per_second\"] = results_raw[\"train_samples_per_second\"]\r\n",
        "    all_results[\"train_steps_per_second\"] = results_raw[\"train_steps_per_second\"]\r\n",
        "    all_results[\"eval_runtime\"] = results_raw[\"eval_runtime\"]\r\n",
        "    all_results[\"eval_samples\"] = results_raw[\"eval_samples\"]\r\n",
        "    all_results[\"eval_samples_per_second\"] = results_raw[\"eval_samples_per_second\"]\r\n",
        "    all_results[\"eval_steps_per_second\"] = results_raw[\"eval_steps_per_second\"]\r\n",
        "    all_results[\"predict_runtime\"] = results_raw[\"predict_runtime\"]\r\n",
        "    all_results[\"predict_samples\"] = results_raw[\"predict_samples\"]\r\n",
        "    all_results[\"predict_samples_per_second\"] = results_raw[\"predict_samples_per_second\"]\r\n",
        "    all_results[\"predict_steps_per_second\"] = results_raw[\"predict_steps_per_second\"]\r\n",
        "\r\n",
        "with open(f\"{model_path}/trainer_state.json\") as f:\r\n",
        "    trainer_state_raw = json.load(f)\r\n",
        "    trainer_state={}\r\n",
        "    trainer_state[\"total_steps\"] = trainer_state_raw[\"max_steps\"]\r\n",
        "    trainer_state[\"total_flos\"] = trainer_state_raw[\"total_flos\"]"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625638912497
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\r\n",
        "# read emissions results\r\n",
        "with open(f\"{model_path}/emissions.csv\") as f:\r\n",
        "    emissions_raw = csv.DictReader(f)\r\n",
        "    emissions={}\r\n",
        "    for row in emissions_raw:\r\n",
        "        emissions[\"timestamp\"] = row[\"timestamp\"]\r\n",
        "        emissions[\"duration\"] = row[\"duration\"]\r\n",
        "        emissions[\"emissions\"] = row[\"emissions\"]\r\n",
        "        emissions[\"energy_consumed\"] = row[\"energy_consumed\"]\r\n",
        "        emissions[\"country_name\"] = row[\"country_name\"]\r\n",
        "        emissions[\"region\"] = row[\"region\"]\r\n",
        "        emissions[\"cloud_provider\"] = row[\"cloud_provider\"]\r\n",
        "        emissions[\"cloud_region\"] = row[\"cloud_region\"]"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625638914050
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_CARD_TEMPLATE = \"\"\"\r\n",
        "---\r\n",
        "language: en\r\n",
        "tags:\r\n",
        "- azureml\r\n",
        "- bart\r\n",
        "- summarization\r\n",
        "license: apache-2.0\r\n",
        "datasets:\r\n",
        "- samsum\r\n",
        "model-index:\r\n",
        "- name: {model_name}\r\n",
        "  results:\r\n",
        "  - task: \r\n",
        "      name: Abstractive Text Summarization\r\n",
        "      type: abstractive-text-summarization\r\n",
        "    dataset:\r\n",
        "      name: \"SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization\" \r\n",
        "      type: samsum\r\n",
        "widget:\r\n",
        "- text: | \r\n",
        "    Henry: Hey, is Nate coming over to watch the movie tonight?\r\n",
        "    Kevin: Yea, he said he'll be arriving a bit later at around 7 since he gets off of work at 6. Have you taken out the garbage yet?\r\n",
        "    Henry: Oh I forgot. I'll do that once I'm finished with my assignment for my math class.\r\n",
        "    Kevin: Yea, you should take it out as soon as possible. And also, Nate is bringing his girlfriend.\r\n",
        "    Henry: Nice, I'm really looking forward to seeing them again.\r\n",
        "---\r\n",
        "\r\n",
        "## `{model_name}`\r\n",
        "This model was trained using Microsoft's `AzureML`. It was fine-tuned on the `SAMSum` corpus from `facebook/bart-large` checkpoint.\r\n",
        "\r\n",
        "More information on the fine-tuning process (includes samples and benchmarks):  \r\n",
        "*(currently still WIP, major updates coming soon: 7/6/21~7/9/21)*\r\n",
        "\r\n",
        "## Resource Usage\r\n",
        "These results are retrieved from AzureML Studio's resource monitoring module. All experiments were ran on AzureML's low priority clusters.\r\n",
        "\r\n",
        "| key | value |\r\n",
        "| --- | ----- |\r\n",
        "| AzureML SKU | ND40rs_v2 (8 X V100 32GB) |\r\n",
        "| Region | US West 2 |\r\n",
        "| Run Duration | X |\r\n",
        "| Compute Cost (LowPriority/Dedicated) | $X/$X (USD) |\r\n",
        "| Average CPU Utilization | X |\r\n",
        "| Average GPU Utilization | X |\r\n",
        "| GPU Memory Usage (Avg/Peak) | X/X (GB) |\r\n",
        "| Total GPU Energy Usage | X (kJ) |\r\n",
        "\r\n",
        "*Compute cost is calculated from run duration and SKU's price per hour. Updated SKU pricing could be found here: https://azure.microsoft.com/en-us/pricing/details/machine-learning/  \r\n",
        "*Peak memory usage is calculated from average peak across all utilized GPUs.  \r\n",
        "\r\n",
        "### Carbon Emissions\r\n",
        "These results are obtained using `codecarbon`. The carbon emission is estimated from training runtime only (excluding setup and evaluation runtime).  \r\n",
        "CodeCarbon: https://github.com/mlco2/codecarbon  \r\n",
        "\r\n",
        "| key | value |\r\n",
        "| --- | ----- |\r\n",
        "{carbon_table}\r\n",
        "\r\n",
        "## Hyperparameters\r\n",
        "```yaml\r\n",
        "fp16: True\r\n",
        "per device batch size: 16\r\n",
        "effective batch size: 128\r\n",
        "epoch: 3.0\r\n",
        "learning rate: 5e-5\r\n",
        "weight decay: 0.1\r\n",
        "seed: 1\r\n",
        "```\r\n",
        "\r\n",
        "## Usage\r\n",
        "```python\r\n",
        "from transformers import pipeline\r\n",
        "summarizer = pipeline(\"summarization\", model=\"henryu-lin/{model_name}\")\r\n",
        "\r\n",
        "conversation = '''Henry: Hey, is Nate coming over to watch the movie tonight?\r\n",
        "    Kevin: Yea, he said he'll be arriving a bit later at around 7 since he gets off of work at 6. Have you taken out the garbage yet?\r\n",
        "    Henry: Oh I forgot. I'll do that once I'm finished with my assignment for my math class.\r\n",
        "    Kevin: Yea, you should take it out as soon as possible. And also, Nate is bringing his girlfriend.\r\n",
        "    Henry: Nice, I'm really looking forward to seeing them again.\r\n",
        "'''\r\n",
        "summarizer(conversation)\r\n",
        "```\r\n",
        "\r\n",
        "## Results\r\n",
        "| ROUGE | Score |\r\n",
        "| ----- | ----- |\r\n",
        "{eval_table}\r\n",
        "{test_table}\r\n",
        "\r\n",
        "| Metric | Value |\r\n",
        "| ------ | ----- |\r\n",
        "{all_table}\r\n",
        "{state_table}\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "# Generate model card\r\n",
        "model_card = MODEL_CARD_TEMPLATE.format(\r\n",
        "    model_name=model_n,\r\n",
        "    eval_table=\"\\n\".join(f\"| {k} | {v} |\" for k, v in eval_results.items()),\r\n",
        "    test_table=\"\\n\".join(f\"| {k} | {v} |\" for k, v in test_results.items()),\r\n",
        "    all_table=\"\\n\".join(f\"| {k} | {v} |\" for k, v in all_results.items()),\r\n",
        "    state_table=\"\\n\".join(f\"| {k} | {v} |\" for k, v in trainer_state.items()),\r\n",
        "    carbon_table=\"\\n\".join(f\"| {k} | {v} |\" for k, v in emissions.items()),\r\n",
        ")\r\n",
        "\r\n",
        "with open(f\"{model_path}/README.md\", \"w\") as f:\r\n",
        "    f.write(model_card)"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625639262595
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}