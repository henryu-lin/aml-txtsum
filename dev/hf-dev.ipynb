{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#transformers 4.6.1\r\n",
        "#pip freeze | cut -d'=' -f1 | xargs -n1 pip install -U\r\n",
        "#nvidia-smi\r\n",
        "#torch.cuda.is_available()\r\n",
        "#torch.cuda.empty_cache()\r\n",
        "import nltk\r\n",
        "#nltk.download('punkt')\r\n",
        "import numpy as np\r\n",
        "#import pandas as pd\r\n",
        "import torch\r\n",
        "import argparse\r\n",
        "import mlflow\r\n",
        "#import azureml.core\r\n",
        "#import logging\r\n",
        "#import sys\r\n",
        "#import time\r\n",
        "import os\r\n",
        "from datasets import load_from_disk, load_metric\r\n",
        "from transformers.integrations import MLflowCallback, AzureMLCallback\r\n",
        "from transformers import (\r\n",
        "    AutoTokenizer,\r\n",
        "    AutoModelForSeq2SeqLM,\r\n",
        "    Seq2SeqTrainingArguments,\r\n",
        "    Seq2SeqTrainer,\r\n",
        "    DataCollatorForSeq2Seq,\r\n",
        "    #set_seed,\r\n",
        "    AutoConfig,\r\n",
        "    #HfArgumentParser\r\n",
        ")\r\n",
        "\r\n",
        "try:\r\n",
        "    nltk.data.find(\"tokenizers/punkt\")\r\n",
        "except (LookupError, OSError):\r\n",
        "    nltk.download(\"punkt\", quiet=True)"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1622002824265
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLflowNoParamCallback(MLflowCallback):\r\n",
        "    \"\"\"\r\n",
        "    A :class:`~transformers.TrainerCallback` that sends the logs to `MLflow <https://www.mlflow.org/>`__.\r\n",
        "    \"\"\"\r\n",
        "    def setup(self, args, state, model):\r\n",
        "        log_artifacts = os.getenv(\"HF_MLFLOW_LOG_ARTIFACTS\", \"FALSE\").upper()\r\n",
        "        if log_artifacts in {\"TRUE\", \"1\"}:\r\n",
        "            self._log_artifacts = True\r\n",
        "        if state.is_world_process_zero:\r\n",
        "            self._ml_flow.start_run()\r\n",
        "            combined_dict = args.to_dict()\r\n",
        "            \"\"\"\r\n",
        "            if hasattr(model, \"config\") and model.config is not None:\r\n",
        "                model_config = model.config.to_dict()\r\n",
        "                combined_dict = {**model_config, **combined_dict}\r\n",
        "            \"\"\"\r\n",
        "            # remove params that are too long for MLflow\r\n",
        "            for name, value in list(combined_dict.items()):\r\n",
        "                # internally, all values are converted to str in MLflow\r\n",
        "                if len(str(value)) > self._MAX_PARAM_VAL_LENGTH:\r\n",
        "                    logger.warning(\r\n",
        "                        f\"Trainer is attempting to log a value of \"\r\n",
        "                        f'\"{value}\" for key \"{name}\" as a parameter. '\r\n",
        "                        f\"MLflow's log_param() only accepts values no longer than \"\r\n",
        "                        f\"250 characters so we dropped this attribute.\"\r\n",
        "                    )\r\n",
        "                    del combined_dict[name]\r\n",
        "            # MLflow cannot log more than 100 values in one go, so we have to split it\r\n",
        "            combined_dict_items = list(combined_dict.items())\r\n",
        "            for i in range(0, len(combined_dict_items), self._MAX_PARAMS_TAGS_PER_BATCH):\r\n",
        "                self._ml_flow.log_params(dict(combined_dict_items[i : i + self._MAX_PARAMS_TAGS_PER_BATCH]))\r\n",
        "        self._initialized = True"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1621837534184
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HfArgumentParser wip\r\n",
        "parser = argparse.ArgumentParser(description=\"HuggingFace Trainer (WIP)\")\r\n",
        "parser.add_argument(\r\n",
        "    \"--data\",\r\n",
        "    type=str,\r\n",
        "    default=None,\r\n",
        "    help=\"data path\")\r\n",
        "# cmd args\r\n",
        "args = parser.parse_args()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download model (cache)\r\n",
        "#model_name = \"google/pegasus-large\"\r\n",
        "#model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir = \"./cache\")\r\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir = \"./cache\")\r\n",
        "\r\n",
        "# save model\r\n",
        "#model.save_pretrained(\"../model/hf-pegasus\")\r\n",
        "#tokenizer.save_pretrained(\"../model/hf-pegasus\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1621686995032
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# td: model -> workspace blob storage -> mount -> compute\r\n",
        "model_path = \"../model/hf-pegasus/\"\r\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n",
        "config = AutoConfig.from_pretrained(model_path)\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\r\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path, config=config).to(device)"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1621836757860
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# td? FileSystems integration for cloud storage (abfs for Azure Blob service)\r\n",
        "# load dataset from workspace\r\n",
        "#data = load_from_disk(args.data, keep_in_memory=True)\r\n",
        "\r\n",
        "data = load_from_disk(\"../data/xsum\", keep_in_memory=True)\r\n",
        "data"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['document', 'id', 'summary'],\n        num_rows: 204045\n    })\n    validation: Dataset({\n        features: ['document', 'id', 'summary'],\n        num_rows: 11332\n    })\n    test: Dataset({\n        features: ['document', 'id', 'summary'],\n        num_rows: 11334\n    })\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1621835725319
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train/eval/test sets\r\n",
        "# max samples wip\r\n",
        "train_data = data[\"train\"].select(range(1000, 1004))\r\n",
        "eval_data = data[\"validation\"].select(range(1000, 1004))\r\n",
        "test_data = data[\"test\"].select(range(1000, 1004))\r\n",
        "train_data[0]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "{'document': 'Media playback is not supported on this device\\nLee, 25, made a course-record 10-under-par 62 in the first round, and is ahead of China\\'s Shanshan Feng and Thailand\\'s Ariya Jutanugarn who are tied second.\\nScotland\\'s Catriona Matthew scored a stunning second-round 65 to go nine under par, two shots behind leader Lee.\\nEngland\\'s Charley Hull is tied in 10th place on five under at Woburn.\\nPlaying at her home event, Hull made five birdies between holes seven and 12, but was put on the clock from 13 to 16 because of slow play.\\nShe told BBC Sport: \"It was raining so I had to rush and bogeyed the 16th hole, it is stuff you have to deal with.\\n\"I am usually a fast player but happy with the way I came through the middle part.\"\\nMeanwhile, England\\'s Bronte Law - the leading amateur who has had to borrow her clubs - carded consecutive rounds of 70 for a place in tied-17th position on four under par.\\nNew Zealand\\'s world number one Lydia Ko made the cut on equal par after a round of 70.\\nMedia playback is not supported on this device\\nWe\\'ve launched a new BBC Sport newsletter, bringing all the best stories, features and video right to your inbox. You can sign up here.',\n 'id': '36927102',\n 'summary': \"South Korea's Mirim Lee maintained her lead at the British Open by one shot after a second-round 71, to go 11 under par at Woburn.\"}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1621835725456
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# max sequence length\r\n",
        "# (xsum: 512, 56), (reddit-tifu: 512, 128)\r\n",
        "max_source_length = 512\r\n",
        "max_target_length = 56\r\n",
        "\r\n",
        "# td parse: source & target columns\r\n",
        "def preprocess_function(examples):\r\n",
        "    inputs = [doc for doc in examples[\"document\"]]\r\n",
        "    model_inputs = tokenizer(inputs, max_length=max_source_length, truncation=True)\r\n",
        "\r\n",
        "    with tokenizer.as_target_tokenizer():\r\n",
        "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\r\n",
        "\r\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\r\n",
        "    return model_inputs\r\n",
        "\r\n",
        "tokenized_train = train_data.map(preprocess_function, batched=True)\r\n",
        "tokenized_eval = eval_data.map(preprocess_function, batched=True)\r\n",
        "tokenized_test = test_data.map(preprocess_function, batched=True)\r\n",
        "\r\n",
        "len(tokenized_train)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9e0930e22e3420faae3264614909f26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f7b10ccf6154f8aa7f54ae91bfa1ed3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1635577cb174ed284fe4449c424e2b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "4"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1621835733530
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# task performance metric\r\n",
        "metric = load_metric(\"rouge\")\r\n",
        "def compute_metrics(eval_pred):\r\n",
        "    predictions, labels = eval_pred\r\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\r\n",
        "\r\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\r\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\r\n",
        "    \r\n",
        "    # rouge sentence newline formatting (data post processing)\r\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\r\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\r\n",
        "    \r\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\r\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\r\n",
        "    \r\n",
        "    # mean summary length metric\r\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\r\n",
        "    result[\"sum_len\"] = np.mean(prediction_lens)\r\n",
        "    \r\n",
        "    return {k: round(v, 4) for k, v in result.items()}\r\n",
        "\r\n",
        "metric"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "Metric(name: \"rouge\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}, usage: \"\"\"\nCalculates average rouge scores for a list of hypotheses and references\nArgs:\n    predictions: list of predictions to score. Each predictions\n        should be a string with tokens separated by spaces.\n    references: list of reference for each prediction. Each\n        reference should be a string with tokens separated by spaces.\n    rouge_types: A list of rouge types to calculate.\n        Valid names:\n        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n        `\"rougeL\"`: Longest common subsequence based scoring.\n        `\"rougeLSum\"`: rougeLsum splits text using `\"\n\"`.\n        See details in https://github.com/huggingface/datasets/issues/617\n    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n    use_agregator: Return aggregates if this is set to True\nReturns:\n    rouge1: rouge_1 (precision, recall, f1),\n    rouge2: rouge_2 (precision, recall, f1),\n    rougeL: rouge_l (precision, recall, f1),\n    rougeLsum: rouge_lsum (precision, recall, f1)\nExamples:\n\n    >>> rouge = datasets.load_metric('rouge')\n    >>> predictions = [\"hello there\", \"general kenobi\"]\n    >>> references = [\"hello there\", \"general kenobi\"]\n    >>> results = rouge.compute(predictions=predictions, references=references)\n    >>> print(list(results.keys()))\n    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n    >>> print(results[\"rouge1\"])\n    AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))\n    >>> print(results[\"rouge1\"].mid.fmeasure)\n    1.0\n\"\"\", stored examples: 0)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1621835738544
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For optimization, both pre-training and fine-tuning used Adafactor (Shazeer & Stern, 2018) with square root learning rate decay and dropout rate of 0.1.\r\n",
        "#num_beams = 8, # PEGASUS -> 8\r\n",
        "#gradient_accumulation_steps = (256/batch_size/nodes) # PEGASUS effective batch size -> 256\r\n",
        "\r\n",
        "# for early stopping callback\r\n",
        "#load_best_model_at_end = True,\r\n",
        "#metric_for_best_model = \"eval_rouge2\",\r\n",
        "#greater_is_better = True,\r\n",
        "\r\n",
        "#freeze_embeds = True,\r\n",
        "batch_size = 4\r\n",
        "train_args = Seq2SeqTrainingArguments(\r\n",
        "    per_device_train_batch_size = batch_size,\r\n",
        "    per_device_eval_batch_size = batch_size,\r\n",
        "    gradient_accumulation_steps = 256 / batch_size,\r\n",
        "    adafactor = True, # PEGASUS -> Adafactor\r\n",
        "    fp16 = False, # hf-PEGASUS ONLY FP32\r\n",
        "    label_smoothing_factor = 0.1,\r\n",
        "    weight_decay = 0.01,\r\n",
        "    learning_rate = 0.0001, # (1e-4) -> 256 effective batch size\r\n",
        "    num_train_epochs = 1.0,\r\n",
        "    evaluation_strategy = \"epoch\",\r\n",
        "    logging_strategy = \"steps\",\r\n",
        "    logging_steps = 1,\r\n",
        "    logging_dir='./logs',\r\n",
        "    save_strategy = \"epoch\",\r\n",
        "    save_total_limit = 1,\r\n",
        "    output_dir = \"./outputs/pegasus-dev\",\r\n",
        "    overwrite_output_dir = True,\r\n",
        "    predict_with_generate = True,\r\n",
        "    #do_eval = True,\r\n",
        "    do_train = True,\r\n",
        "    do_predict = True\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1621835742008
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#optimizers=[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]\r\n",
        "#[torch.optim.Adam(params=model.parameters(), lr=args.learning_rate), None]\r\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\r\n",
        "\r\n",
        "trainer = Seq2SeqTrainer(\r\n",
        "    model,\r\n",
        "    #optimizers = optimizers,\r\n",
        "    args = train_args,\r\n",
        "    train_dataset = tokenized_train,\r\n",
        "    eval_dataset = tokenized_eval,\r\n",
        "    data_collator = data_collator,\r\n",
        "    tokenizer = tokenizer,\r\n",
        "    compute_metrics = compute_metrics\r\n",
        ")\r\n",
        "\r\n",
        "# add EarlyStoppingCallback\r\n",
        "#from transformers import EarlyStoppingCallback\r\n",
        "#trainer.add_callback(EarlyStoppingCallback)"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1621835748790
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# azureml-mlflow param overflow\r\n",
        "trainer.add_callback(MLflowNoParamCallback)\r\n",
        "trainer.remove_callback(MLflowCallback)\r\n",
        "#trainer.remove_callback(AzureMLCallback)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for dev testing\r\n",
        "#torch.cuda.empty_cache()\r\n",
        "mlflow.end_run()"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1621811723044
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate before fine-tuning\r\n",
        "trainer.evaluate(\r\n",
        "    max_length = max_target_length, #data_args.val_max_target_length\r\n",
        "    metric_key_prefix = \"eval\",\r\n",
        "    num_beams = 8 #data_args.num_beams\r\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempted to log scalar metric eval_loss:\n",
            "4.544429779052734\n",
            "Attempted to log scalar metric eval_rouge1:\n",
            "23.1517\n",
            "Attempted to log scalar metric eval_rouge2:\n",
            "7.2789\n",
            "Attempted to log scalar metric eval_rougeL:\n",
            "17.3721\n",
            "Attempted to log scalar metric eval_rougeLsum:\n",
            "18.0478\n",
            "Attempted to log scalar metric eval_sum_len:\n",
            "42.25\n",
            "Attempted to log scalar metric eval_runtime:\n",
            "125.9204\n",
            "Attempted to log scalar metric eval_samples_per_second:\n",
            "0.032\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "{'eval_loss': 4.544429779052734,\n 'eval_rouge1': 23.1517,\n 'eval_rouge2': 7.2789,\n 'eval_rougeL': 17.3721,\n 'eval_rougeLsum': 18.0478,\n 'eval_sum_len': 42.25,\n 'eval_runtime': 125.9204,\n 'eval_samples_per_second': 0.032,\n 'init_mem_cpu_alloc_delta': 8192,\n 'init_mem_cpu_peaked_delta': 0,\n 'eval_mem_cpu_alloc_delta': 139485184,\n 'eval_mem_cpu_peaked_delta': 321056768}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1621835912538
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fine-tune\r\n",
        "trainer.train()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer is attempting to log a value of \"{'summarization_aeslc': {'length_penalty': 0.6, 'max_length': 32, 'max_position_embeddings': 512}, 'summarization_arxiv': {'length_penalty': 0.8, 'max_length': 256, 'max_position_embeddings': 1024}, 'summarization_big_patent': {'length_penalty': 0.7, 'max_length': 256, 'max_position_embeddings': 1024}, 'summarization_billsum': {'length_penalty': 0.6, 'max_length': 256, 'max_position_embeddings': 1024}, 'summarization_cnn_dailymail': {'length_penalty': 0.8, 'max_length': 128, 'max_position_embeddings': 1024}, 'summarization_gigaword': {'length_penalty': 0.6, 'max_length': 32, 'max_position_embeddings': 128}, 'summarization_large': {'length_penalty': 0.8, 'max_length': 256, 'max_position_embeddings': 1024}, 'summarization_multi_news': {'length_penalty': 0.8, 'max_length': 256, 'max_position_embeddings': 1024}, 'summarization_newsroom': {'length_penalty': 0.8, 'max_length': 128, 'max_position_embeddings': 512}, 'summarization_pubmed': {'length_penalty': 0.8, 'max_length': 256, 'max_position_embeddings': 1024}, 'summarization_reddit_tifu': {'length_penalty': 0.6, 'max_length': 128, 'max_position_embeddings': 512}, 'summarization_wikihow': {'length_penalty': 0.6, 'max_length': 256, 'max_position_embeddings': 512}, 'summarization_xsum': {'length_penalty': 0.8, 'max_length': 64, 'max_position_embeddings': 512}}\" for key \"task_specific_params\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute.\n",
            "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/transformers/optimization.py:562: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1616554788289/work/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
            "  exp_avg_sq_row.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-1))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 09:42, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n      <th>Sum Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.074100</td>\n      <td>4.351368</td>\n      <td>22.834600</td>\n      <td>6.981300</td>\n      <td>16.822700</td>\n      <td>18.115700</td>\n      <td>47.000000</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempted to log scalar metric loss:\n",
            "0.0741\n",
            "Attempted to log scalar metric learning_rate:\n",
            "0.0\n",
            "Attempted to log scalar metric epoch:\n",
            "1.0\n",
            "Attempted to log scalar metric eval_loss:\n",
            "4.351367950439453\n",
            "Attempted to log scalar metric eval_rouge1:\n",
            "22.8346\n",
            "Attempted to log scalar metric eval_rouge2:\n",
            "6.9813\n",
            "Attempted to log scalar metric eval_rougeL:\n",
            "16.8227\n",
            "Attempted to log scalar metric eval_rougeLsum:\n",
            "18.1157\n",
            "Attempted to log scalar metric eval_sum_len:\n",
            "47.0\n",
            "Attempted to log scalar metric eval_runtime:\n",
            "554.8311\n",
            "Attempted to log scalar metric eval_samples_per_second:\n",
            "0.007\n",
            "Attempted to log scalar metric epoch:\n",
            "1.0\n",
            "Attempted to log scalar metric train_runtime:\n",
            "649.8689\n",
            "Attempted to log scalar metric train_samples_per_second:\n",
            "0.002\n",
            "Attempted to log scalar metric total_flos:\n",
            "0\n",
            "Attempted to log scalar metric epoch:\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "TrainOutput(global_step=1, training_loss=0.07407309859991074, metrics={'train_runtime': 649.8689, 'train_samples_per_second': 0.002, 'total_flos': 0, 'epoch': 1.0, 'init_mem_cpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 4480581632, 'train_mem_cpu_peaked_delta': 4798541824})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1621809960692
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simple dev test\r\n",
        "results = trainer.predict(\r\n",
        "    tokenized_test,\r\n",
        "    max_length = max_target_length,\r\n",
        "    num_beams = 8)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1620886876371
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# eval\r\n",
        "if train_args.do_eval:\r\n",
        "    #logger.info(\"*** Evaluate ***\")\r\n",
        "\r\n",
        "    metrics = trainer.evaluate(\r\n",
        "        max_length = max_target_length, #data_args.val_max_target_length\r\n",
        "        metric_key_prefix = \"eval\",\r\n",
        "        num_beams = 8 #data_args.num_beams\r\n",
        "    )\r\n",
        "    metrics[\"eval_samples\"] = len(tokenized_eval)\r\n",
        "    #max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\r\n",
        "    #metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\r\n",
        "\r\n",
        "    trainer.log_metrics(\"eval\", metrics)\r\n",
        "    trainer.save_metrics(\"eval\", metrics)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train wip\r\n",
        "if train_args.do_train:\r\n",
        "    checkpoint = None\r\n",
        "    if train_args.resume_from_checkpoint is not None:\r\n",
        "        checkpoint = train_args.resume_from_checkpoint\r\n",
        "    elif last_checkpoint is not None:\r\n",
        "        checkpoint = last_checkpoint\r\n",
        "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\r\n",
        "    trainer.save_model()\r\n",
        "\r\n",
        "    metrics = train_result.metrics\r\n",
        "    metrics[\"train_samples\"] = len(tokenized_train)\r\n",
        "    #max_train_samples = data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\r\n",
        "    #metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\r\n",
        "\r\n",
        "    trainer.log_metrics(\"train\", metrics)\r\n",
        "    trainer.save_metrics(\"train\", metrics)\r\n",
        "    trainer.save_state()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inference summary generation\r\n",
        "if train_args.do_predict:\r\n",
        "    #logger.info(\"*** Predict ***\")\r\n",
        "\r\n",
        "    results = trainer.predict(\r\n",
        "        tokenized_test,\r\n",
        "        metric_key_prefix = \"test\",\r\n",
        "        max_length = max_target_length, #data_args.val_max_target_length\r\n",
        "        num_beams = 8 #data_args.num_beams\r\n",
        "    )\r\n",
        "    metrics = results.metrics\r\n",
        "    metrics[\"test_samples\"] = len(tokenized_test)\r\n",
        "    #max_predict_samples = data_args.max_predict_samples if data_args.max_predict_samples is not None else len(tokenized_test)\r\n",
        "    #metrics[\"predict_samples\"] = min(max_predict_samples, len(tokenized_test))\r\n",
        "\r\n",
        "    trainer.log_metrics(\"test\", metrics)\r\n",
        "    trainer.save_metrics(\"test\", metrics)\r\n",
        "\r\n",
        "    # output text summaries\r\n",
        "    if trainer.is_world_process_zero():\r\n",
        "        if train_args.predict_with_generate:\r\n",
        "            summary_texts = tokenizer.batch_decode(\r\n",
        "                results.predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True # docs\r\n",
        "            )\r\n",
        "            summary_texts = [text.strip() for text in summary_texts] # necessary.?\r\n",
        "            summary_file = os.path.join(train_args.output_dir, \"generated_summaries.txt\")\r\n",
        "            with open(summary_file, \"w\") as writer:\r\n",
        "                writer.write(\"\\n\".join(summary_texts))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "
          },
          "metadata": {}
        }
      ],
      "execution_count": 22,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1621811279717
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# track runtime\r\n",
        "#start_time = time.time()\r\n",
        "# action\r\n",
        "#print(f\"action took {(time.time() - start_time) / 60:.2f}mn\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "mlflow.exceptions.RestException: BAD_REQUEST: Response: {'Error': {'Code': 'UserError', 'Severity': None, 'Message': 'A field of the entity is over the size limit. FieldName=Parameters, Limit=100, Size=148. See https://aka.ms/azure-machine-learning-limits for service limits documentation.', 'MessageFormat': None, 'MessageParameters': None, 'ReferenceCode': None, 'DetailsUri': None, 'Target': None, 'Details': [], 'InnerError': None, 'DebugInfo': None}, 'Correlation': {'operation': '97dc438dde54304987b7615e3ff523fd', 'request': '17d9cfdbe83ad14a'}, 'Environment': 'eastus', 'Location': 'eastus', 'Time': '2021-04-30T16:16:38.6563505+00:00', 'ComponentName': 'mlflow', 'error_code': 'BAD_REQUEST'}"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!az ml job create --file hf-train.yml\r\n",
        "\r\n",
        "# https://github.com/google-research/pegasus/blob/939830367bcf411193d2b5eca2f2f90f3f9260ca/pegasus/params/public_params.py\r\n",
        "# adafactor optimizer blocked (pegasus, t5)\r\n",
        "# deepspeed & fp16 blocked (pegasus)\r\n",
        "#sshleifer/distill-pegasus-xsum-16-4\r\n",
        "#sshleifer/distill-pegasus-xsum-16-8\r\n",
        "#sshleifer/distill-pegasus-xsum-12-12\r\n",
        "#google/pegasus-xsum"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}