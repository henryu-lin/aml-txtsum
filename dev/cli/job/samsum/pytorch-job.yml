$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json
code:
    local_path: ../../src
command: >
    python -m torch.distributed.launch 
    --nproc_per_node 4 trainer-wip.py 
    --model_name_or_path "facebook/bart-large" 
    --config_name "./configs/bart-xsum-config.json" 
    --dataset_name "samsum" 
    --dataset_path {inputs.corpus} 
    --max_source_length 512 
    --max_target_length 90 
    --fp16 True 
    --seed 1 
    --per_device_train_batch_size 8 
    --per_device_eval_batch_size 8 
    --gradient_accumulation_steps 1 
    --learning_rate 5e-5 
    --num_train_epochs 3.0 
    --evaluation_strategy "epoch" 
    --logging_strategy "epoch" 
    --do_train 
    --do_predict 
    --predict_with_generate 
    --overwrite_output_dir 
    --output_dir "./outputs" 
    --logging_dir "./logs" 
    --ddp_find_unused_parameters False
inputs:
    corpus:
        data: azureml:hf-samsum:1
        mode: download
environment: azureml:hf-gpu:1
compute:
    target: azureml:gpu-v100-4
    instance_count: 2
experiment_name: hf-pytorch-demo
description: Finetune an encoder-decoder Transformer (BART) for dialogue summarization (SAMSum) with Hugging Face (PyTorch).