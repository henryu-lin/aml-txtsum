{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "#nltk.download('punkt')\n",
        "import torch\n",
        "import numpy as np\n",
        "import datasets\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    AutoConfig\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1623085403199
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WARNING: long process (xsum, cnn)\r\n",
        "#reddit = load_dataset(\"reddit\", cache_dir=\"../cache\")\r\n",
        "#cnn = load_dataset(\"cnn_dailymail\", \"3.0.0\", cache_dir=\"../cache\")\r\n",
        "#reddit_tifu = load_dataset(\"reddit_tifu\", \"long\", cache_dir=\"../cache\")\r\n",
        "#samsum = load_dataset(\"samsum\", cache_dir=\"../cache\")\r\n",
        "#cnn.save_to_disk(\"../data/cnn-dm\")\r\n",
        "#reddit_tifu.save_to_disk(\"../data/reddit-tifu\")\r\n",
        "#samsum.save_to_disk(\"../data/samsum\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1622247175911
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\", cache_dir = \"./cache\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samsum = load_from_disk(\"../data/samsum\")\n",
        "xsum = load_from_disk(\"../data/xsum\")"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1623086616878
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check sequence length\n",
        "def preprocess_function(examples):\n",
        "    inputs = [doc for doc in examples[\"dialogue\"]]\n",
        "    model_inputs = tokenizer(inputs, return_length=True)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"summary\"], return_length=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    print(\"samples:\", len(model_inputs.length))\n",
        "    print(\"source (avg)\", sum(model_inputs.length)/len(model_inputs.length))\n",
        "    print(\"target (avg)\", sum(labels.length)/len(labels.length))\n",
        "    print(\"source (min)\", min(model_inputs.length), \"(max)\", max(model_inputs.length))\n",
        "    print(\"target (min)\", min(labels.length), \"(max)\", max(labels.length))\n",
        "\n",
        "    return model_inputs"
      ],
      "outputs": [],
      "execution_count": 53,
      "metadata": {
        "gather": {
          "logged": 1623087479533
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_source_length = 512\r\n",
        "max_target_length = 64\r\n",
        "\r\n",
        "def truncate(examples):\r\n",
        "    inputs = [doc for doc in examples[\"dialogue\"]]\r\n",
        "    model_inputs = tokenizer(inputs, max_length=max_source_length, truncation=True, return_overflowing_tokens=True)\r\n",
        "\r\n",
        "    with tokenizer.as_target_tokenizer():\r\n",
        "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True, return_overflowing_tokens=True)\r\n",
        "\r\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\r\n",
        "\r\n",
        "    print(\"source (truncated)\", model_inputs[\"num_truncated_tokens\"])\r\n",
        "    print(\"target (truncated)\", labels[\"num_truncated_tokens\"])\r\n",
        "\r\n",
        "    return model_inputs"
      ],
      "outputs": [],
      "execution_count": 61,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1623087686799
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sam = samsum[\"train\"].map(preprocess_function, batched=True)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/15 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d9295224a724f85b5879cfe9745be69"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "samples: 1000\n",
            "source (avg) 157.728\n",
            "target (avg) 27.438\n",
            "source (min) 20 (max) 638\n",
            "target (min) 7 (max) 75\n",
            "samples: 1000\n",
            "source (avg) 164.006\n",
            "target (avg) 27.968\n",
            "source (min) 17 (max) 703\n",
            "target (min) 3 (max) 75\n",
            "samples: 1000\n",
            "source (avg) 154.585\n",
            "target (avg) 27.289\n",
            "source (min) 17 (max) 821\n",
            "target (min) 6 (max) 90\n",
            "samples: 1000\n",
            "source (avg) 160.552\n",
            "target (avg) 27.309\n",
            "source (min) 21 (max) 773\n",
            "target (min) 6 (max) 74\n",
            "samples: 1000\n",
            "source (avg) 158.821\n",
            "target (avg) 27.719\n",
            "source (min) 22 (max) 1081\n",
            "target (min) 4 (max) 80\n",
            "samples: 1000\n",
            "source (avg) 152.994\n",
            "target (avg) 26.85\n",
            "source (min) 20 (max) 677\n",
            "target (min) 6 (max) 73\n",
            "samples: 1000\n",
            "source (avg) 160.182\n",
            "target (avg) 27.764\n",
            "source (min) 2 (max) 869\n",
            "target (min) 6 (max) 79\n",
            "samples: 1000\n",
            "source (avg) 150.226\n",
            "target (avg) 27.297\n",
            "source (min) 19 (max) 593\n",
            "target (min) 5 (max) 83\n",
            "samples: 1000\n",
            "source (avg) 165.824\n",
            "target (avg) 27.52\n",
            "source (min) 21 (max) 4469\n",
            "target (min) 6 (max) 73\n",
            "samples: 1000\n",
            "source (avg) 161.418\n",
            "target (avg) 28.13\n",
            "source (min) 21 (max) 935\n",
            "target (min) 7 (max) 74\n",
            "samples: 1000\n",
            "source (avg) 156.176\n",
            "target (avg) 27.441\n",
            "source (min) 19 (max) 761\n",
            "target (min) 7 (max) 77\n",
            "samples: 1000\n",
            "source (avg) 161.023\n",
            "target (avg) 27.412\n",
            "source (min) 24 (max) 788\n",
            "target (min) 6 (max) 76\n",
            "samples: 1000\n",
            "source (avg) 158.166\n",
            "target (avg) 27.487\n",
            "source (min) 22 (max) 923\n",
            "target (min) 7 (max) 82\n",
            "samples: 1000\n",
            "source (avg) 157.636\n",
            "target (avg) 27.541\n",
            "source (min) 21 (max) 699\n",
            "target (min) 7 (max) 81\n",
            "samples: 732\n",
            "source (avg) 163.71448087431693\n",
            "target (avg) 27.3224043715847\n",
            "source (min) 21 (max) 740\n",
            "target (min) 7 (max) 76\n"
          ]
        }
      ],
      "execution_count": 64,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1623087879567
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = xsum[\"test\"].map(preprocess_function, batched=True)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/12 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33b36d6355dd4cc0a389e922b948d865"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "samples: 1000\n",
            "source (avg) 499.319\n",
            "target (avg) 28.096\n",
            "source (min) 2 (max) 3175\n",
            "target (min) 6 (max) 77\n",
            "samples: 1000\n",
            "source (avg) 509.236\n",
            "target (avg) 27.97\n",
            "source (min) 46 (max) 3174\n",
            "target (min) 7 (max) 65\n",
            "samples: 1000\n",
            "source (avg) 488.139\n",
            "target (avg) 28.197\n",
            "source (min) 5 (max) 4531\n",
            "target (min) 7 (max) 70\n",
            "samples: 1000\n",
            "source (avg) 493.184\n",
            "target (avg) 28.07\n",
            "source (min) 29 (max) 3696\n",
            "target (min) 7 (max) 71\n",
            "samples: 1000\n",
            "source (avg) 483.254\n",
            "target (avg) 28.012\n",
            "source (min) 9 (max) 2463\n",
            "target (min) 5 (max) 76\n",
            "samples: 1000\n",
            "source (avg) 488.888\n",
            "target (avg) 28.083\n",
            "source (min) 24 (max) 15278\n",
            "target (min) 6 (max) 88\n",
            "samples: 1000\n",
            "source (avg) 500.922\n",
            "target (avg) 28.123\n",
            "source (min) 14 (max) 4095\n",
            "target (min) 8 (max) 78\n",
            "samples: 1000\n",
            "source (avg) 489.854\n",
            "target (avg) 28.51\n",
            "source (min) 24 (max) 3379\n",
            "target (min) 9 (max) 76\n",
            "samples: 1000\n",
            "source (avg) 496.445\n",
            "target (avg) 28.162\n",
            "source (min) 30 (max) 5596\n",
            "target (min) 6 (max) 103\n",
            "samples: 1000\n",
            "source (avg) 479.006\n",
            "target (avg) 27.871\n",
            "source (min) 17 (max) 2783\n",
            "target (min) 7 (max) 73\n",
            "samples: 1000\n",
            "source (avg) 479.467\n",
            "target (avg) 28.238\n",
            "source (min) 25 (max) 3450\n",
            "target (min) 8 (max) 77\n",
            "samples: 334\n",
            "source (avg) 487.0898203592814\n",
            "target (avg) 28.820359281437124\n",
            "source (min) 29 (max) 3197\n",
            "target (min) 11 (max) 63\n"
          ]
        }
      ],
      "execution_count": 45,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1623086960178
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reddit\r\n",
        "# features: ['author', 'subreddit', 'summary', 'text']\r\n",
        "# num_rows: 3848330\r\n",
        "#reddit = reddit[\"train\"].remove_columns([\"id\", \"subreddit_id\", \"body\", \"normalizedBody\"])\r\n",
        "#reddit = reddit.rename_column(\"content\", \"text\")\r\n",
        "#reddit.save_to_disk(\"../data/reddit\")\r\n",
        "#reddit = load_from_disk(\"../data/reddit\")\r\n",
        "#reddit"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1620687309179
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reddit-dota\r\n",
        "# features: ['author', 'summary', 'text']\r\n",
        "# num_rows: 22366\r\n",
        "#dota = reddit.filter(lambda x: x[\"subreddit\"] == \"DotA2\")\r\n",
        "#dota = dota.remove_columns(\"subreddit\")\r\n",
        "\r\n",
        "# bad text/summary @ dota[9]\r\n",
        "#dota = dota.filter(lambda x: len(x[\"text\"]) > len(x[\"summary\"])) # source length > target length\r\n",
        "#dota.flatten_indices().save_to_disk(\"../data/reddit-dota\")\r\n",
        "dota = load_from_disk(\"../data/reddit-dota\")\r\n",
        "dota"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1620765475840
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reddit-dota-proc\r\n",
        "# num_rows: 19949\r\n",
        "#max_text_length = 50000\r\n",
        "min_summary_length = 20\r\n",
        "max_ratio = 0.5\r\n",
        "min_ratio = 0.005\r\n",
        "#dota = dota.filter(lambda x: len(x[\"text\"]) <= max_text_length)\r\n",
        "proc = dota.filter(lambda x: len(x[\"summary\"]) >= min_summary_length)\r\n",
        "proc = proc.filter(lambda x: len(x[\"summary\"])/len(x[\"text\"]) <= max_ratio)\r\n",
        "proc = proc.filter(lambda x: len(x[\"summary\"])/len(x[\"text\"]) >= min_ratio)\r\n",
        "# bertscore filter?\r\n",
        "proc.flatten_indices().save_to_disk(\"../data/reddit-dota-proc\")\r\n",
        "proc"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1620765545591
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reddit-dota-qtr\r\n",
        "# num_rows: 14389\r\n",
        "max_text_length = 3200\r\n",
        "min_summary_length = 30\r\n",
        "# 0.25(max: 128/512) & 0.125(max: 64/512)\r\n",
        "max_ratio = 0.25\r\n",
        "min_ratio = 0.01\r\n",
        "dota = load_from_disk(\"../data/reddit-dota-proc\")\r\n",
        "dota = dota.filter(lambda x: len(x[\"text\"]) <= max_text_length)\r\n",
        "dota = dota.filter(lambda x: len(x[\"summary\"]) >= min_summary_length)\r\n",
        "dota = dota.filter(lambda x: len(x[\"summary\"])/len(x[\"text\"]) <= max_ratio)\r\n",
        "dota = dota.filter(lambda x: len(x[\"summary\"])/len(x[\"text\"]) >= min_ratio)\r\n",
        "dota.flatten_indices().save_to_disk(\"../data/reddit-dota-qtr\")\r\n",
        "dota = load_from_disk(\"../data/reddit-dota-qtr\")\r\n",
        "dota"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1620765612229
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}