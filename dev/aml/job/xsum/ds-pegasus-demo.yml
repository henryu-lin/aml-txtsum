$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json
experiment_name: xsum
tags:
    model: "pegasus-large"
    seed: "1"
    learning_rate: "1e-4"
    batch_size: "256"
    deepspeed: "true"
    zero_offload: "false"
code:
    local_path: ../../src
command: >
    deepspeed trainer-wip.py 
    --deepspeed "./configs/ds_zero2_adafactor.json" 
    --model_name_or_path "google/pegasus-large" 
    --config_name "./configs/pegasus-xsum-config.json" 
    --dataset_name "xsum" 
    --dataset_path {inputs.corpus} 
    --max_source_length 512 
    --max_target_length 64 
    --fp16 False 
    --adafactor True 
    --seed 1 
    --per_device_train_batch_size 8 
    --per_device_eval_batch_size 16 
    --gradient_accumulation_steps 4 
    --learning_rate 1e-4 
    --weight_decay 0.01 
    --label_smoothing_factor 0.1 
    --num_train_epochs 5.0 
    --freeze_embeds False 
    --freeze_encoder False 
    --do_train 
    --do_eval 
    --do_predict 
    --save_strategy "no" 
    --predict_with_generate 
    --overwrite_output_dir 
    --output_dir "./outputs" 
    --logging_dir "./logs" 
environment: azureml:hf-deepspeed:3
inputs:
    corpus:
        data: azureml:hf-xsum:1
        mode: download
compute:
    target: azureml:gpu-v100-8-lp
    instance_count: 1