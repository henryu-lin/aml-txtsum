$schema: https://azuremlschemas.azureedge.net/latest/sweepJob.schema.json
experiment_name: hf-sweep
description: bart samsum lr sweep (deepspeed)
type: sweep_job
algorithm: grid
trial:
    code: 
        local_path: ../../src
    command: >
        deepspeed trainer-wip.py 
        --deepspeed "./ds_config_zero2.json" 
        --model_name_or_path "facebook/bart-large" 
        --config_name "./bart-xsum-config.json" 
        --dataset_name "samsum" 
        --dataset_path {inputs.corpus} 
        --max_source_length 512 
        --max_target_length 90 
        --fp16 True 
        --seed 322 
        --sortish_sampler False 
        --per_device_train_batch_size 16 
        --per_device_eval_batch_size 16 
        --gradient_accumulation_steps 1 
        --learning_rate {search_space.lr} 
        --num_train_epochs 3.0 
        --evaluation_strategy "epoch" 
        --logging_strategy "epoch" 
        --freeze_embeds True 
        --freeze_encoder False 
        --train_early_stopping True 
        --early_stopping_patience 3 
        --early_stopping_threshold 0.1 
        --load_best_model_at_end True 
        --metric_for_best_model "eval_rouge1" 
        --greater_is_better True 
        --do_train 
        --do_predict 
        --predict_with_generate 
        --overwrite_output_dir 
        --output_dir "./outputs/hf-model" 
        --logging_dir "./logs"
    inputs:
        corpus:
            data: azureml:hf-samsum:1
            mode: download
    environment: azureml:hf-deepspeed:2
    compute:
        target: azureml:gpu-v100-8-lp
        instance_count: 1
search_space:
    lr:
        type: choice
        values: [0.00002, 0.00004, 0.00006, 0.00008, 0.0001]
objective:
    primary_metric: eval_rouge1
    goal: maximize
max_total_trials: 4
max_concurrent_trials: 1
timeout_minutes: 60