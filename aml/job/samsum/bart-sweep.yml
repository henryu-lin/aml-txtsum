$schema: https://azuremlschemas.azureedge.net/latest/sweepJob.schema.json
experiment_name: hf-sweep
type: sweep_job
algorithm: grid
trial:
    code: 
        local_path: ../../src
    command: >
        deepspeed trainer-wip.py 
        --deepspeed "./configs/ds_zero2.json" 
        --model_name_or_path "facebook/bart-large" 
        --config_name "./configs/bart-xsum-config.json" 
        --dataset_name "samsum" 
        --dataset_path {inputs.corpus} 
        --max_source_length 512 
        --max_target_length 90 
        --fp16 True 
        --seed 1 
        --per_device_train_batch_size 16 
        --per_device_eval_batch_size 16 
        --gradient_accumulation_steps 1 
        --learning_rate {search_space.lr} 
        --num_train_epochs 3.0 
        --evaluation_strategy "epoch" 
        --logging_strategy "epoch" 
        --freeze_embeds False 
        --freeze_encoder False 
        --do_train 
        --do_predict 
        --predict_with_generate 
        --overwrite_output_dir 
        --output_dir "./outputs/hf-model" 
        --logging_dir "./logs" 
    environment: azureml:hf-deepspeed:2
    inputs:
        corpus:
            data: azureml:hf-samsum:1
            mode: download
    compute:
        target: azureml:gpu-v100-8-lp
        instance_count: 1
    distribution:
        type: pytorch
        process_count: 8
search_space:
    lr:
        type: choice
        values: [0.00002, 0.00003, 0.00005, 0.00008, 0.0001]
objective:
    primary_metric: eval_rouge1
    goal: maximize
max_total_trials: 5
max_concurrent_trials: 1
timeout_minutes: 120