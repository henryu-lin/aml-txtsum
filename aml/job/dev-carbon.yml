$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json
experiment_name: hf-dev
description: bart-xsum config
code:
    local_path: ../../src
command: >
    python trainer-wip.py 
    --model_name_or_path "facebook/bart-large" 
    --config_name "./bart-xsum-config.json" 
    --dataset_name "xsum" 
    --dataset_path {inputs.corpus} 
    --max_source_length 512 
    --max_target_length 64 
    --ddp_find_unused_parameters False 
    --fp16 True 
    --fp16_opt_level "02" 
    --fp16_full_eval True 
    --seed 322 
    --sortish_sampler True 
    --max_eval_samples 256 
    --max_predict_samples 256 
    --per_device_train_batch_size 4 
    --per_device_eval_batch_size 8 
    --gradient_accumulation_steps 1 
    --learning_rate 3e-5 
    --max_steps 30 
    --evaluation_strategy "steps" 
    --logging_strategy "steps" 
    --eval_steps 10 
    --logging_steps 5 
    --save_total_limit 10 
    --train_early_stopping True 
    --early_stopping_patience 3 
    --early_stopping_threshold 0.1 
    --load_best_model_at_end True 
    --metric_for_best_model "eval_rouge2" 
    --greater_is_better True 
    --freeze_embeds True 
    --freeze_encoder False 
    --do_train 
    --do_predict 
    --predict_with_generate 
    --overwrite_output_dir 
    --output_dir "./outputs/hf-model" 
    --logging_dir "./logs" 
environment: azureml:hf-carbon:1
inputs:
    corpus:
        data: azureml:hf-xsum:1
        mode: download
compute:
    target: azureml:gpu-v100-4
    instance_count: 1
distribution:
    type: pytorch
    process_count: 4