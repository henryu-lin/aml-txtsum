$schema: https://azuremlschemas.azureedge.net/latest/sweepJob.schema.json
type: sweep_job
algorithm: grid
trial:
    code: 
        local_path: src
    command: >
        python -m torch.distributed.launch
        --nproc_per_node 8 main.py
        --model_name_or_path "facebook/bart-large"
        --config_name "./configs/hf-bart-xsum.json"
        --dataset_name "samsum"
        --dataset_path {inputs.corpus}
        --max_source_length 512
        --max_target_length 90
        --fp16 True
        --seed 1
        --per_device_train_batch_size 16
        --per_device_eval_batch_size 16
        --gradient_accumulation_steps 1
        --learning_rate {search_space.lr}
        --num_train_epochs 3.0
        --weight_decay {search_space.wd}
        --evaluation_strategy "epoch"
        --logging_strategy "epoch"
        --do_train
        --do_predict
        --predict_with_generate
        --overwrite_output_dir
        --output_dir "./outputs"
        --logging_dir "./outputs/runs"
        --ddp_find_unused_parameters False
    inputs:
        corpus:
            data: azureml:hf-samsum:1
            mode: download
    environment: azureml:hf-zero-gpu:1
    compute:
        target: azureml:gpu-v100-8
        instance_count: 1
search_space:
    lr:
        type: choice
        values: [0.00003, 0.00005]
    wd:
        type: choice
        values: [0.0, 0.1]
objective:
    primary_metric: eval_rouge1
    goal: maximize
max_total_trials: 4
max_concurrent_trials: 1
timeout_minutes: 60
experiment_name: hf-sweep-demo
description: Hyperparameter tune an encoder-decoder Transformer model (BART) for dialogue summarization (SAMSum) with a grid search sweep job.
