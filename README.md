# aml-hf-txtsum
WIP (by June 28th?):
-Hugging Face AML sample guides (scripts & notebook)
Similar to the Hugging Face Amazon SageMaker guides
https://huggingface.co/transformers/master/sagemaker.html
https://huggingface.co/blog/sagemaker-distributed-training-seq2seq
-Will link this repository & guides in trained models pages for exposure once basic sample is completed
https://huggingface.co/henryu-lin/bart-large-samsum
-Repo code cleanup for public release

Extras (by June 30th?):
-Modified Hugging Face experiment summarization trainer documentation
-Retrieve & compile core experiment benchmark results & comparisons (with resource/carbon metrics & cost)
PyTorch Baseline, sparse attention, parameter freezing, DeepSpeed (ZeRO 2, ZeRO 3, CPU Offload)
*If these experiments are worthwhile, I'll finish evaluations & do a proper paper/blog write-up..
